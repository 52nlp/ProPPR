include ../Makefile.in

### DATASET
NAME=uml_1
TRAIN=${NAME}-train
TEST=${NAME}-test

SRW=l2p:mu=0.0001:eta=1.0
EPOCHS=100
PROVER=dpr
APR=eps=0.0001:alph=0.001

ifeq ($(strip $(ITERS)),)
ITERS=10
endif

### TARGETS
# To run fully, do
#     $ make isg
#     $ make

all: results.txt

include ../common.in

clean:
	rm -f *results.txt *.grounded *.solutions.txt params.*.wts *.gradient \
	$(foreach ruletype,h22 delta alone,$(addprefix *_$(ruletype)_*,.ppr .wam))

.PRECIOUS: %.examples params.wts %.solutions.txt

## Below adapted from /remote/curtis/yww/data/forKatie/uml/Makefile
## March 2015

# This project iteratively refines a program based on h22.ppr. At each
# iteration, the rulefile ${NAME}_h22_XX.ppr is formed by
# concatenating h22.ppr and those rules generated by the previous
# iteration.
#
# At iteration t, we perform (t-1) epochs of training, then take the
# gradient of the loss. Additional rules are generated based on
# features with a negative gradient. These additional rules are stored
# in ${NAME}_delta_$t.ppr, and we proceed until no new rules are added.
#
# We measure the performance after each iteration by evaluating
# ${NAME}_delta_$t.ppr -- which does not include the h22 rules -- on the
# test queries.


# Iteration 0 is the baseline, without any additional rules.
#
# Iteration 1 is the SG method, which uses only one gradient
# calculation to compute rules.
#
# All other iterations form the ISG method, which iterates on the
# gradient until no new rules are added.

structureLearning: $(foreach it,$(wildcard ${NAME}_delta_*.ppr),$(addsuffix $(subst .ppr,.solutions.txt,$(subst _delta,-test.alone,$(it))),pre. post.))

results.txt: $(foreach it,$(wildcard ${NAME}_delta_*.ppr),$(addsuffix $(subst .ppr,.results.txt,$(subst _delta,-test.alone,$(it))),pre. post.))
	echo phase.subset.iteration uR mR uMRR mMRR uMAP mMAP > $@
	cat $^ >> $@

%.results.txt: %.solutions.txt
	ROOT=$*;\
	ROOT=$${ROOT#*.};\
	ROOT=$${ROOT%.*};\
	python ${PROPPR}/scripts/answermetrics.py --data $${ROOT}.examples --answers $< --metric recall --metric mrr --metric map |\
	grep -e "micro:" -e "macro:" |\
	awk '{print $$3}' |\
	tr "\n" " " |\
	awk '{name="$*"; gsub("\."," ",name); print name,$$0}' > $@
	cat $@

# The common target definitions are insufficient for our needs,
# since the program will differ for each iteration. Thus we define our
# own solutions targets, including the program dependency for that
# iteration.

pre.${NAME}-test.%.solutions.txt: ${NAME}-test.examples ${NAME}_%.wam
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.QueryAnswerer --programFiles ${NAME}-test.cfacts:$(word 2,$^) --queries $< --solutions $@ --prover ${PROVER} --threads ${THREADS} --apr ${APR} ${UNNORMALIZED}

post.${NAME}-test.%.solutions.txt: ${NAME}-test.examples ${NAME}_%.wam params.%.wts
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.QueryAnswerer --programFiles ${NAME}-test.cfacts:$(word 2,$^) --queries $< --solutions $@ --prover ${PROVER} --threads ${THREADS} --apr ${APR} ${UNNORMALIZED} --params $(word 3,$^)

params.%.wts: ${TRAIN}.%.examples.grounded
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.Trainer --train $< --params $@ --threads ${THREADS} --srw ${SRW} --epochs ${EPOCHS} --apr ${APR}

${NAME}-train.%.examples.grounded: ${NAME}-train.examples ${NAME}_%.wam
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.Grounder --programFiles ${NAME}-train.cfacts:$(word 2,$^) --queries $< --grounded $@ --prover ${PROVER} --threads ${THREADS} --apr ${APR}

# Additional targets for building an executable program from gradient-generated rules:

${NAME}_h22_%.ppr: ${NAME}_delta_%.ppr h22.ppr
	cat $^ > $@

${NAME}_alone_%.ppr: ${NAME}_delta_%.ppr interp.ppr
	cat $^ > $@

# Actually run the iterated structural gradient procedure, and supporting target:

isg:
	python scripts/iterativeGradientFinder.py ${NAME} ${ITERS}

${NAME}_%.gradient: ${NAME}-train.h22_%.examples.grounded
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.GradientFinder --grounded $< --threads ${THREADS} --apr ${APR} --epochs $* --srw ${SRW} --gradient $@